# Distributed In-Memory Cache (Redis Clone)

A high-performance, thread-safe distributed key-value store implemented in pure Python. Designed to mirror the internal architecture of **Redis**, featuring **LRU eviction**, **Probabilistic TTL expiration**, **Async Replication**, and **Horizontal Sharding**.

Achieves **1.2 million operations per second** with **<1Âµs latency** in local benchmarks.

---

## Key Features

* **Core Caching:** O(1) `GET`, `SET`, and `DELETE` operations.
* **Eviction Policies:**
* **FIFO (First-In-First-Out):** Basic queue-based eviction.
* **LRU (Least Recently Used):** Custom implementation using Doubly Linked List + Hash Map (via `OrderedDict`) for O(1) tracking.


* **Expiration (TTL):** "Redis-style" probabilistic active expiration. A background thread samples random keys to expire, ensuring memory is freed without "stop-the-world" latency spikes.
* **Thread Safety:** Fine-grained locking (`threading.RLock`) ensures data consistency under high concurrency.
* **Distributed Architecture:**
* **Sharding:** Horizontal scaling using Stable Hashing (CRC32) to distribute keys across multiple nodes.
* **Replication:** Master-Slave architecture with non-blocking asynchronous replication queues.



---

## Architecture

The system is built in tiers, each adding complexity and capability:

1. **Tier 1: SimpleCache** - Basic dictionary wrapper with size enforcement.
2. **Tier 2: LRUCache** - Adds access tracking for intelligent eviction.
3. **Tier 3: TTLCache** - Adds time-based expiry and background cleanup threads.
4. **Tier 4: CacheNode** - Adds network-ready Master/Slave roles and replication logic.
5. **Tier 5: CacheCluster** - The client-side router that shards data across multiple nodes.

### System Topology

```mermaid
graph TD
    Client[Client Application] --> Router[Cluster Router (CRC32 Sharding)]
    Router -->|Shard 0| Node0[Master Node 0]
    Router -->|Shard 1| Node1[Master Node 1]
    Router -->|Shard 2| Node2[Master Node 2]
    Router -->|Shard 3| Node3[Master Node 3]

    Node0 -.->|Async Repl| Replica0[Replica Node 0]
    Node1 -.->|Async Repl| Replica1[Replica Node 1]

```

---

## Performance Benchmarks

Benchmarks run on a local development environment (Apple M-Series / Intel i7 equivalent):

| Metric | Result | Target | Status |
| --- | --- | --- | --- |
| **Throughput** | **1,207,668 OPS** | > 100k OPS | âœ… PASS |
| **Read Latency** | **0.83 Âµs** | < 50 Âµs | âœ… PASS |
| **Hit Rate** | **100%** | N/A | âœ… PASS |

*Note: Benchmarks performed with 80% Read / 20% Write workload.*

---

## Installation & Usage

### Prerequisites

* Python 3.7+ (No external dependencies required)

### Quick Start

Clone the repository and run the interactive demo:

```bash
git clone https://github.com/yourusername/distributed-cache.git
cd distributed-cache
python demo.py

```

### Example Usage

```python
from distributed_cache import CacheCluster

# Initialize a cluster with 4 shards
cluster = CacheCluster(num_shards=4, shard_size=100_000)

# Set a value (Router handles sharding automatically)
cluster.set("user:1001", {"name": "Alice", "role": "Engineer"})

# Get a value
user = cluster.get("user:1001")
print(user) 
# Output: {'name': 'Alice', 'role': 'Engineer'}

```

---

## Technical Deep Dive

### 1. The "O(1)" Challenge

Standard Python dictionaries are fast, but maintaining a strict `MAX_SIZE` with FIFO or LRU eviction typically requires  operations (scanning the list). I solved this using `collections.OrderedDict`, which internally maintains a Doubly Linked List. This allows `popitem(last=False)` to execute in true  time.

### 2. Probabilistic Cleanup

Deleting all expired keys at once freezes the database. Instead, I implemented a "Sampling Strategy":

* Every 1 second, sample 20 random keys.
* If >25% are expired, repeat immediately (aggressive mode).
* If <25% are expired, sleep (relaxed mode).
* *Result:* Memory is freed effectively with zero impact on P99 latency.

### 3. Stable Sharding

Instead of Python's built-in `hash()` (which is randomized per process), I used `zlib.crc32`. This ensures that `user:1001` always maps to `Shard #2`, regardless of server restarts or deployments.

---

## Future Improvements

* **Consistent Hashing:** Upgrade from Modulo Sharding to a Hash Ring to allow dynamic node scaling with minimal data movement.
* **Persistence:** Implement AOF (Append Only File) or RDB snapshots to save data to disk.
* **TCP Server:** Wrap the library in a TCP socket server (using `socket` or `asyncio`) to allow remote connections (making it a true Redis alternative).

---

## ðŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.
